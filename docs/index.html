<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>NeuroSeg Meets DINOv3</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

  <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css">

  <style>
    :root {
      --page-width: 1080px;
      --text-color: #222222;
      --muted-text: #666666;
      --light-gray: #f5f5f7;
      --section-padding-y: 40px;
    }

    * {
      box-sizing: border-box;
      margin: 0;
      padding: 0;
    }

    body {
      font-family: -apple-system, BlinkMacSystemFont, "SF Pro Text", system-ui,
        -system-ui, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      background: #ffffff;
      color: var(--text-color);
      line-height: 1.6;
      -webkit-font-smoothing: antialiased;
    }

    .page {
      max-width: var(--page-width);
      margin: 32px auto 80px auto;
      padding: 0 24px;
    }

    /* ----- header ----- */

    .hero {
      text-align: center;
      padding: 40px 0 8px 0;
    }

    .hero-title {
      font-size: 40px;
      font-weight: 700;
      letter-spacing: 0.02em;
      line-height: 1.22;
      margin-bottom: 12px;
    }

    .hero-subtitle {
      font-size: 16px;
      color: var(--muted-text);
      max-width: 780px;
      margin: 0 auto;
    }

    /* ---- hero buttons (Paper / Code) ---- */
    .hero-buttons {
      margin-top: 18px;
      display: inline-flex;
      gap: 12px;
    }
  
    /* .hero-btn {
      display: inline-flex;
      align-items: center;
      justify-content: center;
      height: 40px;
      padding: 0 18px;
      border-radius: 999px;
      background: #2f2f2f;
      color: #ffffff;
      text-decoration: none;
      font-size: 14px;
      font-weight: 600;
      letter-spacing: 0.01em;
      box-shadow: 0 1px 0 rgba(0,0,0,0.08); 
    } */

    .hero-btn{
      display: inline-flex;
      align-items: center;
      justify-content: center;
      height: 44px;          /* Á®çÂæÆÈ´ò‰∏ÄÁÇπÊõ¥ÂÉèÊåâÈíÆ */
      padding: 0 22px;       /* Â∑¶Âè≥Êõ¥ÂÆΩ -> ÊåâÈíÆ‰∏çÁü≠ */
      border-radius: 999px;
      background: #2f2f2f;
      color: #ffffff;
      text-decoration: none;
      font-size: 16px;       /* Â≠óÁï•Â§ß‰∏ÄÁÇπ */
      font-weight: 600;
      letter-spacing: 0.01em;
      box-shadow: 0 1px 0 rgba(0,0,0,0.08);
      gap: 10px;             /* ‚úÖ Áî® gap ÊéßÂà∂ icon ÂíåÊñáÂ≠óË∑ùÁ¶ªÔºàÊØî margin-right Êõ¥Á®≥ÂÆöÔºâ */
    }
  
    .hero-btn:hover {
      background: #1f1f1f;
    }
  
    /* disabled state: not clickable for now */
    .hero-btn.disabled {
      opacity: 0.85;
      cursor: default;
      pointer-events: none;
    }

    /* .hero-btn .icon {
      display: inline-flex;
      align-items: center;
      margin-right: 8px;
      font-size: 16px; /* ÂõæÊ†áÂ§ßÂ∞è */
      line-height: 1;   
    }    */

    /* Êó¢ÁÑ∂Áî®‰∫Ü gapÔºåÂ∞±‰∏çË¶ÅÂÜçÁî® margin-right */
    .hero-btn .icon{
      display: inline-flex;
      align-items: center;
      font-size: 18px;
      line-height: 1;
      margin-right: 0;
    } 

    /* ----- basic section layout ----- */

    section {
      padding: var(--section-padding-y) 0;
    }

    section:nth-of-type(odd) {
      background: #ffffff;
    }

    section:nth-of-type(even) {
      background: var(--light-gray);
    }

    .section-inner {
      max-width: var(--page-width);
      margin: 0 auto;
      padding: 0 24px;
    }

    /* .section-title {
      text-align: center;
      font-size: 26px;
      font-weight: 600;
      margin-bottom: 32px;
    } */ 
    
    .section-title {
      text-align: center;
      font-size: 32px;      
      font-weight: 700;      
      color: #363636;        
      margin-bottom: 20px;   
      letter-spacing: 0.01em;
    }

    .section-text {
      font-size: 16px;
      color: var(--text-color);
      max-width: 820px;
      margin: 0 auto;
      text-align: justify;
    }

    .section-text p + p {
      margin-top: 14px;
    }

    /* ----- image blocks ----- */

    .figure-wrapper {
      max-width: 900px;
      margin: 0 auto;
      text-align: center;
    }

    /* .figure-frame {
      border-radius: 8px;
      border: 1px solid #e2e2e7;
      background: #ffffff;
      padding: 12px 12px 8px 12px;
      box-shadow: 0 2px 4px rgba(15, 23, 42, 0.04);
    }

    .figure-frame img {
      display: block;
      width: 100%;
      height: auto;
    } */

    .figure-caption {
      font-size: 13px;
      color: var(--muted-text);
      margin-top: 6px;
      text-align: left;
    }

    #teaser{
      padding-top: 8px;
      padding-bottom: 36px;
    }

    #quantitative .section-text {
      margin-bottom: 32px;
    }

    #results .section-text {
      margin-bottom: 32px;
    }

    /* small screen tweaks */
    @media (max-width: 768px) {
      .hero-title {
        font-size: 30px;
      }
      .hero {
        padding-top: 24px;
      }
      section {
        padding: 48px 0;
      }
    }
  </style>
</head>
<body>
  <div class="hero">
    <div class="page">
      <h1 class="hero-title">
        NeuroSeg Meets DINOv3: Transferring 2D Self-Supervised Visual Priors
        to 3D Neuron Segmentation via DINOv3 Initialization
      </h1>
      <p class="hero-subtitle">
        A framework for bringing powerful 2D DINOv3 ConvNeXt representations into 3D neuron
        segmentation, improving thin-structure reconstruction and label efficiency.
      </p>
      
      <!-- <div class="hero-buttons">
        <a class="hero-btn disabled" href="#">
          üìÑ Paper
        </a>
        <a class="hero-btn disabled" href="#">
          üíª Code
        </a>
      </div> -->

      <div class="hero-buttons">
        <a class="hero-btn" href="https://arxiv.org/abs/XXXX.XXXXX" target="_blank" rel="noopener noreferrer">
          <span class="icon" aria-hidden="true"><i class="ai ai-arxiv"></i></span>
          <span>Paper</span>
        </a>
      
        <a class="hero-btn" href="https://github.com/yourname/yourrepo" target="_blank" rel="noopener noreferrer">
          <span class="icon" aria-hidden="true"><i class="fab fa-github"></i></span>
          <span>Code</span>
        </a>
      </div>

    </div>
  </div>

  <!-- Teaser -->
  <section id="teaser">
    <div class="section-inner">
      <div class="figure-wrapper">
        <!-- <div class="figure-frame"> -->
          <!-- replace teaser.png with your actual teaser image file -->
          <img src="assets/images/teaser_img.png" style="width: 60%;" alt="NeurINO teaser figure" />
        </div>
      </div>
    <!-- </div> -->
  </section>

  <!-- Abstract -->
  <section id="abstract">
    <div class="section-inner">
      <h2 class="section-title">Abstract</h2>
      <div class="section-text">
        <p>
          2D visual foundation models, such as DINOv3, a self-supervised model trained on large-scale natural images, 
          have demonstrated strong zero-shot generalization, capturing both rich global context and fine-grained structural cues. 
          However, an analogous 3D foundation model for downstream volumetric neuroimaging remains lacking, 
          largely due to the challenges of 3D image acquisition and the scarcity of high-quality annotations. 
          To address this gap, we propose to adapt the 2D visual representations learned by DINOv3 to a 3D biomedical segmentation model, 
          enabling more data-efficient and morphologically faithful neuronal reconstruction. 
          Specifically, we design an inflation-based adaptation strategy that inflates 2D filters into 3D operators, 
          preserving semantic priors from DINOv3 while adapting to 3D neuronal volume patches. 
          In addition, we introduce a topology-aware skeleton loss to explicitly enforce structural fidelity of graph-based neuronal arbor reconstruction.
          Extensive experiments on four neuronal imaging datasets, including two from BigNeuron and two public datasets, NeuroFly and CWMBS, 
          demonstrate consistent improvements in reconstruction accuracy over SoTA methods, with average gains of 2.9% in Entire Structure Average, 
          2.8% in Different Structure Average, and 3.8% in Percentage of Different Structure.
        </p>
      </div>
    </div>
  </section>

  <!-- Overview of Our Framework -->
  <section id="method">
    <div class="section-inner">
      <h2 class="section-title">Overview of Our Framework</h2>
      <div class="figure-wrapper" style="margin-bottom: 28px;">
        <!-- <div class="figure-frame"> -->
          <!-- replace method.png with your method / pipeline image -->
          <img src="assets/images/method.png" style="width: 100%;" alt="Method / framework diagram" />
          <div class="figure-caption">
            Overview of the framework: We adapt a pretrained DINOv3 ConvNeXt encoder for volumetric neuron segmentation through an inflation-based 3D adaptation strategy, 
            which spatially expands 2D convolutional kernels into 3D while preserving pretrained semantics. 
            The inflated encoder is coupled with a symmetric MedNeXt-style decoder to recover fine-grained neuronal morphology. 
            Multi-level outputs are supervised jointly with the proposed Topology-Aware Skeleton Loss (TASL), 
            which measures structural discrepancies between predicted and ground-truth skeleton graphs.
          </div>
        </div>
      </div>

      <!-- <div class="section-text">
        <p>
          The framework starts from a pretrained 2D DINOv3 ConvNeXt encoder. We inflate convolution
          kernels into 3D and adapt downsampling and normalization to operate on volumetric inputs.
          The encoder is followed by a MedNeXt-style 3D decoder with skip connections, enabling
          fine-grained reconstruction of neurites and branch points.
        </p>
        <p>
          We further study different initialization and freezing strategies for the encoder,
          inflation variants, and normalization replacements to stabilize training on sparse labels
          while preserving the rich visual priors of DINOv3.
        </p>
      </div> -->
    <!-- </div> -->
  </section>

  <!-- Results -->
  <!-- Quantitative Results -->
  <section id="quantitative">
    <div class="section-inner">
      <h2 class="section-title">Quantitative Results</h2>
  
      <!-- <div class="section-text">
        <p>
          Quantitative comparison on multiple neuron datasets for both segmentation
          and tracing. We report standard voxel-wise metrics and topology-aware
          reconstruction scores, showing consistent improvements of <strong>NeurINO</strong>
          over strong baselines.
        </p>
      </div> -->
  
      <!-- Segmentation table -->
      <div class="figure-wrapper" style="margin-bottom: 32px;">
        <img src="assets/images/segmentation.png"
             style="width: 100%;"
             alt="Segmentation quantitative results" />
      </div>
  
      <!-- Tracing table -->
      <div class="figure-wrapper">
        <img src="assets/images/tracing.png"
             style="width: 100%;"
             alt="Tracing quantitative results" />
      </div>
    </div>
  </section>

  <section id="results">
    <div class="section-inner">
      <h2 class="section-title">Qualitative Results</h2>

      <div class="section-text">
        <p>
          Qualitative comparison on multiple neuron datasets. 
          For each figure, the top row presents raw images and segmentation outputs, 
          followed by two rows showing the corresponding reconstruction results generated by SmartTracing and NeuTube. 
          <span style="color: magenta; font-weight: 700;">Magenta boxes</span> indicate severe false negatives (missed neurites) in other methods.
          <span style="color: magenta; font-weight: 700;">Magenta arrows</span> highlight severe false positives in other method.
        </p>
      </div>

      <div class="figure-wrapper" style="margin-bottom: 24px;">
        <!-- <div class="figure-frame"> -->
          <!-- replace results.png with qualitative / quantitative results figure -->
          <img src="assets/images/Fly.png" style="width: 100%;" alt="Drosophila Results" />
          <div class="figure-caption">
            Visualization comparison of the segmentation and tracing results on the Drosophila dataset.
            <!-- The top row presents raw images and segmentation outputs, 
            followed by two rows showing the corresponding reconstruction results generated by SmartTracing and NeuTube. 
            <span style="color: magenta; font-weight: 700;">Magenta boxes</span> indicate severe false negatives (missed neurites) in other methods. -->
          </div>
        </div>

      <!-- Mouse -->
        <div class="figure-wrapper" style="margin-bottom: 40px;">
          <img src="assets/images/Mouse.png" style="width: 100%;" alt="Mouse Results" />
          <div class="figure-caption">
            Visualization comparison of the segmentation and tracing results on the Mouse dataset.
          </div>
        </div>

      <!-- NeuroFly -->
        <div class="figure-wrapper" style="margin-bottom: 40px;">
          <img src="assets/images/NeuroFly.png" style="width: 100%;" alt="NeuroFly Results" />
          <div class="figure-caption">
            Visualization comparison of the segmentation and tracing results on the NeuroFly dataset.
          </div>
        </div>

      <!-- CWMBS -->
        <div class="figure-wrapper" style="margin-bottom: 40px;">
          <img src="assets/images/CWMBS.png" style="width: 100%;" alt="CWMBS Results" />
          <div class="figure-caption">
            Visualization comparison of the segmentation and tracing results on the CWMBS dataset.
          </div>
        </div>
      </div>

      <!-- <div class="section-text">
        <p>
          Quantitative and qualitative results on multiple neuron datasets. 
          The framework
          improves segmentation accuracy and continuity of thin neurites compared to baseline 3D
          models.Across three neuron imaging datasets, the framework consistently outperforms strong 3D
          baselines in both voxel-wise metrics and topology-aware measures. The gains are most
          pronounced under low-label settings, highlighting the benefit of transferring 2D
          self-supervised priors.
        </p>
      </div> -->
    <!-- </div> -->
  </section>
</body>
</html>

